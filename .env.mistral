# ============================================
# MISTRAL-OPTIMIZED CONFIGURATION
# ============================================
# Copy these settings to your .env file for best results with Mistral

# ===== LLM PROVIDER =====
LLM_PROVIDER=ollama
OLLAMA_MODEL=mistral-nemo:latest
OLLAMA_API_URL=http://localhost:11434

# ===== MISTRAL-OPTIMIZED SETTINGS =====
# Temperature: 0.1 (very low for consistent JSON output)
LLM_TEMPERATURE=0.1

# Max tokens: 4096 (plenty for complex SQL queries)
LLM_MAX_TOKENS=4096

# ===== DATABASE CONFIGURATION =====
DB_TYPE=postgres
DB_NAME=your_database
DB_USER=your_user
DB_PASSWORD=your_password
DB_HOST=localhost
DB_PORT=5432

# ===== OUTPUT DATABASE =====
OUTPUT_DB_TYPE=postgres
OUTPUT_DB_NAME=postgres
OUTPUT_DB_USER=postgres
OUTPUT_DB_PASSWORD=your_password
OUTPUT_DB_HOST=localhost

# ===== METADATA =====
DATAMODEL_PATH=./datamodel.json
META_STORAGE=postgres

# ===== VECTOR STORE =====
CHROMA_PATH=./faiss_base
EMBEDDING_MODEL_NAME=sentence-transformers/paraphrase-multilingual-mpnet-base-v2
VECTOR_MODE=faiss

# ===== FRONTEND =====
# Set to 'false' to disable frontend (API only mode)
ENABLE_FRONTEND=true

# ============================================
# IMPORTANT NOTES FOR MISTRAL:
# ============================================
# 1. Temperature MUST be low (0.1) for reliable JSON output
# 2. Use mistral-nemo:latest - it's the best variant
# 3. If you get JSON parsing errors:
#    - Run: python quick_fix.py
#    - Or manually set LLM_TEMPERATURE=0.05 (even lower)
# 4. Max tokens should be at least 2048, preferably 4096
# 5. Make sure Ollama is running: ollama serve
# 6. Pull the model first: ollama pull mistral-nemo:latest
